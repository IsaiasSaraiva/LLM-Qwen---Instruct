ğŸ¤– LLM - Qwen/Qwen2.5-7B-Instruct ğŸš€

ğŸ” Sobre o Projeto Este repositÃ³rio fornece um ambiente otimizado para executar o modelo Qwen/Qwen2.5-7B-Instruct, um modelo leve e eficiente projetado para tarefas de inferÃªncia com desempenho otimizado. Com suporte a CUDA, bitsandbytes, accelerate e outras otimizaÃ§Ãµes, vocÃª pode rodar este modelo com maior eficiÃªncia em sua GPU. ğŸ’¡âš¡

ğŸš€ CaracterÃ­sticas

âœ… Modelo: Qwen/Qwen2.5-7B-Instruct ğŸ¤– âœ… Frameworks: PyTorch + Transformers + Accelerate + bitsandbytes âœ… OtimizaÃ§Ãµes: Suporte a quantizaÃ§Ã£o 4-bit para reduzir uso de memÃ³ria âœ… Compatibilidade: GPUs NVIDIA com suporte a CUDA âœ… InferÃªncia eficiente e rÃ¡pida

ğŸ› ï¸ ConfiguraÃ§Ã£o e InstalaÃ§Ã£o

ğŸ”¹ 1. Clone o RepositÃ³rio

git clone https://github.com/IsaiasSaraiva/LLM-Qwen---Instruct.git

ğŸ”¹ 2. Crie um Ambiente Virtual (Recomendado)

python -m venv venv source venv/bin/activate # Linux/Mac venv\Scripts\activate # Windows

ğŸ”¹ 3. Instale as DependÃªncias

pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 pip install transformers accelerate bitsandbytes

ğŸ”¹ 4. Baixe o Modelo

from transformers import AutoModelForCausalLM, AutoTokenizer

id_model = "Qwen/Qwen2.5-7B-Instruct"

model = AutoModelForCausalLM.from_pretrained(id_model, device_map="auto", load_in_4bit=True) tokenizer = AutoTokenizer.from_pretrained(id_model)

ğŸ”¹ 5. Rode um Exemplo

input_text = "Explique a teoria da evoluÃ§Ã£o de forma simples." inputs = tokenizer(input_text, return_tensors="pt").to("cuda") output = model.generate(**inputs, max_length=200) print(tokenizer.decode(output[0], skip_special_tokens=True))
